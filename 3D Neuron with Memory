import numpy as np
from scipy.special import sph_harm

class Neuron:
    def __init__(self, position, learning_rate=0.01):
        """Initializes a Neuron object with 3D position and weights based on spherical harmonics."""
        self.position = np.array(position)
        self.weights = {
            'pre_training': self._calculate_spherical_harmonics(),
            'input': self._calculate_spherical_harmonics(),
            'long_term': np.zeros_like(self._calculate_spherical_harmonics()),
            'q_learning': self._calculate_spherical_harmonics()
        }
        self.bias = np.random.rand()
        self.type = None
        self.activation = self.sigmoid
        self.learning_rate = learning_rate
        self.last_input = None

    def _calculate_spherical_harmonics(self):
        """Calculates spherical harmonics based on neuron position."""
        x, y, z = self.position
        r = np.sqrt(x**2 + y**2 + z**2)
        if r == 0:
            return np.array([0., 0., 0.])
        theta = np.arccos(z / r)
        phi = np.arctan2(y, x)
        l = 1  # Adjust l for more complex directional sensitivity
        harmonics = np.array([
            sph_harm(-1, l, phi, theta).real,
            sph_harm(0, l, phi, theta).real,
            sph_harm(1, l, phi, theta).real
        ])
        return harmonics

    def set_type(self, type_name):
        """Sets the type of the neuron."""
        self.type = type_name

    def process_input(self, input_value):
        """Processes the input."""
        if self.type is None:
            raise ValueError("Neuron type must be set.")

        used_weights = self.weights.get(self.type, self.weights['input'])
        self.last_input = input_value
        weighted_sum = np.dot(input_value, used_weights) + self.bias
        output = self.activation(weighted_sum)
        return output

    def update_long_term_weights(self, input_value, learning_signal):
        """Updates long-term weights."""
        alpha = 0.1
        self.weights['long_term'] = (1 - alpha) * self.weights['long_term'] + alpha * input_value * learning_signal

    def update_q_learning_weights(self, state, action, reward, next_state, next_max_q, discount_factor=0.9):
        """Updates Q-learning weights."""
        current_q = np.dot(state, self.weights['q_learning'])
        target_q = reward + discount_factor * next_max_q
        delta = target_q - current_q
        self.weights['q_learning'] += self.learning_rate * delta * state

    def set_activation_function(self, activation_function):
        """Sets the activation function."""
        self.activation = activation_function

    def backward(self, error):
        """Performs backpropagation for the input weights, incorporating spherical harmonics."""
        if self.last_input is None:
            raise ValueError("No input processed yet. Call process_input first.")

        if self.activation == self.sigmoid:
            derivative = self.sigmoid_derivative(self.process_input(self.last_input))
        elif self.activation == self.relu:
            derivative = self.relu_derivative(self.process_input(self.last_input))
        else:
            raise ValueError("Derivative for given activation not defined.")

        directional_influence = self._calculate_spherical_harmonics()
        weight_gradients = error * derivative * self.last_input * directional_influence
        bias_gradient = error * derivative

        self.weights['input'] += self.learning_rate * weight_gradients
        self.bias += self.learning_rate * bias_gradient
        return weight_gradients

    @staticmethod
    def sigmoid(x):
        """Sigmoid activation function."""
        return 1 / (1 + np.exp(-x))

    @staticmethod
    def sigmoid_derivative(output):
        """Derivative of the sigmoid function."""
        return output * (1 - output)

    @staticmethod
    def relu(x):
        """ReLU activation function."""
        return np.maximum(0, x)

    @staticmethod
    def relu_derivative(output):
        """Derivative of the ReLU activation function."""
        return np.where(output > 0, 1, 0)

    def get_weights(self, weight_type):
        """Returns the specified weights."""
        return self.weights.get(weight_type)

# Example Usage (Two Neurons):
input_neuron = Neuron(position=(1, 0, 0))  # Example position
input_neuron.set_type('input')
output_neuron = Neuron(position=(0, 1, 0))  # Example position
output_neuron.set_type('output')

input_data = np.array([0.5, 0.2, 0.8])
target_output = 0.7

# Forward pass
intermediate_output = input_neuron.process_input(input_data)
final_output = output_neuron.process_input(intermediate_output)

# Calculate initial error
error = target_output - final_output
print(f"Initial Error: {error}")

# Backward pass
output_gradients = output_neuron.backward(error)
input_neuron.backward(output_gradients)

# Forward pass again to see the effect of backpropagation
intermediate_output = input_neuron.process_input(input_data)
final_output = output_neuron.process_input(intermediate_output)
error = target_output - final_output
print(f"Error After Backpropagation: {error}")

print("Input Neuron Weights (after backprop):", input_neuron.get_weights('input'))
print("Output Neuron Weights (after backprop):", output_neuron.get_weights('input'))
